from airflow.models.baseoperator import BaseOperator
from airflow.hooks.base import BaseHook
from airflow.utils.context import Context
from plugins.utils.log_helper import get_logger
from plugins.utils.save_utils import extract_authors
from plugins.utils.data4library_schema import Data4LibraryLoanItemValidator

import os
import json
import glob
import zipfile
import pendulum
import pymysql


class Data4LibraryFileSaveToDBOperator(BaseOperator):
    template_fields = ("endpoint",)  # âœ… í…œí”Œë¦¿ ë Œë”ë§ ì ìš© ëŒ€ìƒ í•„ë“œ ì§€ì •

    def __init__(self, endpoint: str, mysql_conn_id: str, base_dir: str = "/opt/airflow/files/data4library", **kwargs):
        super().__init__(**kwargs)
        self.endpoint = endpoint
        self.mysql_conn_id = mysql_conn_id
        self.base_dir = base_dir

    def execute(self, context: Context):
        # âœ… self.endpoint ê°’ì€ í…œí”Œë¦¿ì´ ìë™ ë Œë”ë§ëœ ìƒíƒœë¡œ ë“¤ì–´ì˜´
        run_time = pendulum.now().format('YYYYMMDDTHHmmss')
        log_dir = os.path.join(self.base_dir, self.endpoint, f"save_to_db_{run_time}")
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(log_dir, f"save_to_db_{run_time}.log")
        logger = get_logger(self.task_id, log_dir, run_time, log_file=log_file)

        logger.info(f"ğŸ”¥ ìµœì¢… endpoint: {self.endpoint}")
        logger.info(f"íŒŒì¼ íƒìƒ‰ ì‹œì‘: {self.base_dir}/{self.endpoint}")
        json_files = glob.glob(os.path.join(self.base_dir, self.endpoint, "**", "*.json"), recursive=True)
        logger.info(f"ë°œê²¬ëœ JSON íŒŒì¼ ìˆ˜: {len(json_files)}")

        logger.info(f"MySQL ì—°ê²° ì‹œë„: conn_id={self.mysql_conn_id}")
        conn = BaseHook.get_connection(self.mysql_conn_id)
        db = pymysql.connect(
            host=conn.host,
            port=conn.port or 3306,
            user=conn.login,
            password=conn.password,
            database=conn.schema,
            charset='utf8mb4'
        )
        cursor = db.cursor()
        logger.info("MySQL ì—°ê²° ì„±ê³µ")

        sql_path = '/opt/airflow/sql/insert_loan_item_srch.sql'
        with open(sql_path, 'r', encoding='utf-8') as f:
            insert_sql = f.read()

        all_rows = []
        fail_rows = []
        fail_msgs = []
        total_rows = 0
        for file_path in json_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                req = data['response']['request']
                startDt = req['startDt']
                endDt = req['endDt']
                age = req['age']
                docs = data['response']['docs']
                for doc_wrap in docs:
                    doc = doc_wrap['doc']
                    yyyymm = startDt[:4] + startDt[5:7]
                    id_val = f"{yyyymm}_{doc.get('isbn13', '')}_{doc.get('addition_symbol', '')}"
                    row = {
                        'id': id_val,
                        'startDt': startDt,
                        'endDt': endDt,
                        'age': age,
                        'ranking': doc.get('ranking', 0) if doc.get('ranking') else None,
                        'bookname': doc.get('bookname'),
                        'authors': extract_authors(doc.get('authors')),
                        'publisher': doc.get('publisher'),
                        'publication_year': doc.get('publication_year'),
                        'isbn13': doc.get('isbn13'),
                        'addition_symbol': doc.get('addition_symbol'),
                        'vol': doc.get('vol'),
                        'class_no': doc.get('class_no'),
                        'class_nm': doc.get('class_nm'),
                        'bookImageURL': doc.get('bookImageURL'),
                        'bookDtlUrl': doc.get('bookDtlUrl'),
                        'loan_count': int(doc.get('loan_count', 0)) if doc.get('loan_count') else None
                    }
                    total_rows += 1
                    valid, err = Data4LibraryLoanItemValidator.validate_row(row)
                    if valid:
                        all_rows.append(row)
                    else:
                        fail_rows.append(row)
                        fail_msgs.append(f"id={row['id']} - {err}")
            except Exception as e:
                logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path} - {e}")

        fail_count = len(fail_rows)
        success_count = len(all_rows)
        fail_rate = (fail_count / total_rows) * 100 if total_rows > 0 else 0
        logger.info(f"ê²€ì¦ ê²°ê³¼: ì „ì²´ {total_rows}ê±´, ì„±ê³µ {success_count}ê±´, ì‹¤íŒ¨ {fail_count}ê±´, ì‹¤íŒ¨ìœ¨ {fail_rate:.2f}%")
        if fail_count > 0:
            logger.warning(f"ê²€ì¦ ì‹¤íŒ¨ row ì˜ˆì‹œ: {fail_msgs[:5]}")
        if fail_rate >= 70:
            logger.error(f"ì‹¤íŒ¨ìœ¨ {fail_rate:.2f}%ë¡œ ì ì¬ ì¤‘ë‹¨!")
            raise Exception(f"ì ì¬ ì¤‘ë‹¨: ì‹¤íŒ¨ìœ¨ {fail_rate:.2f}%")
        elif fail_count > 0:
            logger.warning(f"ì‹¤íŒ¨ìœ¨ {fail_rate:.2f}%: ì¼ë¶€ rowë§Œ ì ì¬ ì§„í–‰")

        if all_rows:
            try:
                cursor.executemany(insert_sql, all_rows)
                db.commit()
                logger.info(f"DB ì ì¬ ì„±ê³µ: {len(all_rows)}ê±´")
            except Exception as e:
                logger.error(f"DB bulk insert ì‹¤íŒ¨: {e}")
        else:
            logger.warning("ì ì¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        cursor.close()
        db.close()

        logger.info(f"íŒŒì¼ ì••ì¶• ì‹œì‘: {self.base_dir}/{self.endpoint}")
        save_date = pendulum.now().format('YYYYMMDD')
        zip_path = os.path.join(self.base_dir, self.endpoint, f"save_to_db_{save_date}.zip")
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in json_files:
                arcname = os.path.relpath(file_path, os.path.join(self.base_dir, self.endpoint))
                zipf.write(file_path, arcname)
        logger.info(f"ì••ì¶• íŒŒì¼ ìƒì„± ì™„ë£Œ: {zip_path}")
        logger.info("ì „ì²´ ì‘ì—… ì™„ë£Œ")